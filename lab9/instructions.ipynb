{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMA-ES vs Gradient methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cma\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "This notebook aims to explore how the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a powerful gradient-free optimization algorithm, can be used to \"train\" the weights of a simple neural network. We will apply it to a binary classification problem.\n",
    "\n",
    "The primary goal is not to showcase CMA-ES as the best method for standard neural network training. Instead, it's to:\n",
    "\n",
    "- Demonstrate that it can optimize neural network weights by directly optimizing for a metric like accuracy (which can be seen as a non-differentiable objective from a certain perspective).\n",
    "- Critically compare its performance (accuracy, computational effort) against a standard gradient-based training method (like Adam, as used in scikit-learn's MLPClassifier).\n",
    "- Encourage you to be \"suspicious\" and ask critical questions when you encounter non-gradient methods being used for standard, differentiable neural network training tasks where gradient-based methods are well-established and highly efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Dataset: Two Moons\n",
    "We'll use the \"Two Moons\" dataset, a classic non-linearly separable binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.25, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, edgecolors=\"k\", label=\"Train\"\n",
    ")\n",
    "plt.scatter(\n",
    "    X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, marker=\"x\", s=80, label=\"Test\"\n",
    ")\n",
    "plt.title(\"Two Moons Dataset (Standardized)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training set shape: X={X_train_scaled.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set shape: X={X_test_scaled.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Network Definition\n",
    "We will use a simple feedforward neural network with:\n",
    "\n",
    "Input layer: 2 neurons (for our 2D data) <br>\n",
    "Hidden layer: N_HIDDEN neurons with tanh activation. You can start with `N_HIDDEN=32` <br>\n",
    "Output layer: 1 neuron with sigmoid activation (for binary classification probability) <br>\n",
    "The parameters (weights and biases) of this network will be flattened into a single vector for CMA-ES to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUT_NODES = X_train_scaled.shape[1]\n",
    "N_HIDDEN_NODES = 128\n",
    "N_OUTPUT_NODES = 1\n",
    "\n",
    "# Calculate the total number of parameters (D)\n",
    "# Weights W1: N_INPUT x N_HIDDEN\n",
    "# Biases b1: N_HIDDEN\n",
    "# Weights W2: N_HIDDEN x N_OUTPUT\n",
    "# Biases b2: N_OUTPUT\n",
    "D = (N_INPUT_NODES * N_HIDDEN_NODES) + N_HIDDEN_NODES + \\\n",
    "    (N_HIDDEN_NODES * N_OUTPUT_NODES) + N_OUTPUT_NODES\n",
    "\n",
    "print(f\"Neural Network Architecture:\")\n",
    "print(f\"Input Layer: {N_INPUT_NODES} neurons\")\n",
    "print(f\"Hidden Layer: {N_HIDDEN_NODES} neurons (tanh activation)\")\n",
    "print(f\"Output Layer: {N_OUTPUT_NODES} neuron (sigmoid activation)\")\n",
    "print(f\"Total number of parameters (weights & biases) to optimize: D = {D}\")\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def nn_forward_pass(params_vector: np.ndarray, X_data: np.ndarray) -> np.ndarray:\n",
    "    idx_end_W1 = N_INPUT_NODES * N_HIDDEN_NODES\n",
    "    W1 = params_vector[0:idx_end_W1].reshape(N_INPUT_NODES, N_HIDDEN_NODES)\n",
    "\n",
    "    idx_end_b1 = idx_end_W1 + N_HIDDEN_NODES\n",
    "    b1 = params_vector[idx_end_W1:idx_end_b1]\n",
    "\n",
    "    idx_end_W2 = idx_end_b1 + (N_HIDDEN_NODES * N_OUTPUT_NODES)\n",
    "    W2 = params_vector[idx_end_b1:idx_end_W2].reshape(N_HIDDEN_NODES, N_OUTPUT_NODES)\n",
    "\n",
    "    b2 = params_vector[idx_end_W2:]\n",
    "\n",
    "    # Hidden layer\n",
    "    hidden_layer_input = np.dot(X_data, W1) + b1\n",
    "    hidden_layer_output = tanh(hidden_layer_input)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training with CMA-ES\n",
    "We have to define a fitness function that CMA-ES will try to minimize. Since our goal is to maximize accuracy, our fitness will be 1.0 - accuracy.\n",
    "\n",
    "The accuracy is calculated by:\n",
    "- Performing a forward pass with the given parameters.\n",
    "- Thresholding the sigmoid output (e.g., > 0.5 means class 1, else class 0).\n",
    "- Comparing predicted classes to true labels.\n",
    "- This direct use of accuracy (based on a hard threshold) makes the objective effectively non-differentiable with respect to the network parameters if one were to try to use calculus directly without a surrogate like cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Boundary (CMA-ES)\n",
    "Let's see how the decision boundary learned by the CMA-ES-trained network looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(pred_func, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    h = .02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z_proba = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = (Z_proba > 0.5).astype(int)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1 (scaled)\")\n",
    "    plt.ylabel(\"Feature 2 (scaled)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "cma_pred_func = lambda x_data: nn_forward_pass(best_params_cma, x_data)\n",
    "\n",
    "plot_decision_boundary(cma_pred_func, X_train_scaled, y_train, \"Decision Boundary - CMA-ES Trained NN (Train Data)\")\n",
    "plot_decision_boundary(cma_pred_func, X_test_scaled, y_test, \"Decision Boundary - CMA-ES Trained NN (Test Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training with a Standard Gradient-Based Method\n",
    "Now, let's train a neural network with the same architecture using a standard gradient-based optimizer. You can use `scikit-learn`'s MLPClassifier, which employs optimizers like 'adam' or 'sgd' and uses a differentiable loss function (like cross-entropy) internally. Display decision boundary for a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Compare the performance of CMA-ES and MLPClassifier across varying hidden layer sizes (e.g., 8, 16, 32, 64). Evaluate both methods in terms of training time and the quality of the resulting models, using appropriate performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Improve the hyperparameters of the MLPClassifier, with particular attention to the learning rate, the maximum number of iterations, and other relevant training parameters. Your objective is to achieve performance that surpasses that of the CMA-ES baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Read the article titled \"Artyku≈Ç\" available on UPeL, and provide a critical analysis of its content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
