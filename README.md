# Stochastic Methods in Machine Learning

This repository contains laboratory materials for the "Stochastic Methods in Machine Learning" course at AGH University of Science and Technology in Krakow.

## Course Description

This course explores various problems at the intersection of optimization and machine learning.

## Labs Overview

| Lab | Title | Description |
|-----|-------|-------------|
| 1 | Gradient Descent | Implements gradient descent algorithm to train a linear regression model from scratch. |
| 2 | Gradient Descent Extensions | Covers gradient descent extensions including Momentum, AdaGrad, and Adam. Students test these optimizers on standard benchmark test functions: Sphere, Rosenbrock, Rastrigin. |
| 3 | Adversarial Examples | Investigates the vulnerability of neural networks to adversarial attacks, implementing the Fast Gradient Sign Method (FGSM) to generate perturbations that cause misclassification. |
| 4 | Model-Based Offline Optimization | Explores optimization of black-box functions using pre-collected datasets without additional function evaluations. Involves training neural network surrogate models to approximate benchmark functions and implementing gradient-based optimization techniques on these surrogate models to find optimal solutions. |
| 5 | Hyperparameter Optimization | Utilizes Optuna framework to fine-tune CatBoost model hyperparameters on the Covertype dataset. Demonstrates practical application of HPO to maximize classification performance in a multiclass problem. |
| 6 | Bayesian Optimization | Covers various acquisition functions (e.g., Expected Improvement, UCB), focusing on how they manage the exploration-exploitation trade-off. |
| 7 | Normal Distribution | Explores the fundamental properties and significance of the Normal Distribution. |
| 8 | CMA-ES | Applies the pycma library to evolve policy parameters and solve the OpenAI Gym CartPole balancing task. |
| 9 | Neuroevolution | Demonstrates evolutionary training of neural networks by applying CMA-ES to directly optimize network weights for solving a supervised learning problem. |
| 10 | Differential Evolution | Implements Differential Evolution from scratch. |
| 11 | LLM Ã— EA | Evolution of Heuristics |
| 12 | Cuckoo Search | Explores the common issue of seemingly new optimization algorithms that may not offer genuinely novel ideas, focusing on Cuckoo Search and its critical analysis. |

## Issues and Contributions

If you find any mistakes or have suggestions for improvements:

1. **Create an Issue**: Open a new issue in the repository describing the problem or suggestion in detail.
2. **Submit a Pull Request**: If you have a fix or improvement, feel free to fork the repository and submit a pull request with your changes.

Your contributions help improve the quality of these materials for all students.


